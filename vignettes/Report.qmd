---
title: "Report"
author: "Kate Li and Sang Truong"
date: "2024-12-8"
output: html_document
vignette: >
  %\VignetteIndexEntry{Report}
  %\VignetteEngine{quarto::html}
  %\VignetteEncoding{UTF-8}
---

## Package Purpose & Problem Solving

The computational humanities is interested in empirically assessing trends and methods of representation in literature, in ways that traditional close-reading methods cannot. However, as the number of texts subject to purview (that is, as the size of one's "corpus") grows, research methods abstract enough to assess trends across a corpus in its entirety tend to lose the precision required for divulging the granularity of individual texts. Our package aims to address this issue by providing functions that allow users to 1) analyze character development at the textual level, as well as 2) analyzing *trends* in character development at the corpus level. This package does so through the presumption that users already have access to a corpus and have run NLP pipelines (in particular, BookNLP) on the corpus; this package takes in, wrangles, and visualizes data associated with BookNLP parses on a given corpus, allowing users to more directly visualize how individual characters are characterized, and what that might say about thematic trends within their selected corpus. 

# Description of Main Functionality / Features

This package's main functions include reading in BookNLP-parses of a corpus (stored in the file *read_corpus*), wrangling and processing this data to generate key attributes for every character in every document (stored in the file *rsdb*), conducting a range of textual analyses on these attributes such as sentiment analysis, co-reference against a webscraped database on racializing terms, collocate analysis (as in sentiment.R and analysis_funcs.R), and visualization of the resulting output. This host of functions aims to enable users to generate key findings on the nature of characterization in their corpus, beyond the higher-level linguistic patterns that standard text mining methods could produce. 

# Usage of Key Package Functions

The recommended pipeline for analysis is as follows:

### 1) Read in BookNLP Parses & Generate Character Data 
Given a modest corpus (e.g., like one stored under inst/extdata/examplecorpus) containing documents parsed by BookNLP (e.g., JSON files ending with the file extension ".book"), we recommend first running the function "getbook" on the corpus. 

This function will return a list of dataframes containing data on a text's characters (where each dataframe corresponds to one text). Data includes a list of actions performed by the character, done by the character, a list of their possessions, as well as modifiers used to describe them. These dataframes can be either saved to the user's local desktop, or used for downstream analysis (see following steps).

The following functions are not sequential: users can choose whichever function to run in whichever order, depending on their research interests.

### 2) Analyze Trends in Racialization

The function "grab_racialmods" takes in character-level data at the text-level: it accepts one dataframe returned by "getbook" (e.g., one entry in the list output). Users can choose to loop grab_racialmods to apply the function to all texts within their corpus, but the function is currently limited to accepting only one document at a time as an input parameter. 

This function cross-references the modifiers used to describe characters against an existing database containing a list of racializing terms (e.g., ethnographic markers, racial slurs, etc). Users can obtain a list of racializing terms that are used within a given document, and the characters that these terms were used to describe. 

### 3) Sentiment Analysis

The function "grab_sentiment" takes in a list of dataframes containing character data (e.g., the output of "getbooks") and returns a list of modified dataframes, where added variables describe the sentiment of each character's descriptions/actions. Users can choose to perform sentiment analysis on either characters' actions, modifiers, possessions, etc., by specifying one of four parameters in the function (see /man for function documentation). 

Users can run "plot_sentiment" on the output generated by grab_sentiment to visualize the results of the former (e.g., plot_sentiment(grab_sentiment(), measure = mean, charalevel = 5). It generates simple scatter plots relating the majorness of characters in the corpus, to the manners in which they are sentimentally described. Since plot_sentiment takes the output of grab_sentiment() as an input, we recommend running grab_sentiment() prior to running plot_sentiment. 

### 4) TF-IDF Analysis for Character Insights

The function `run_tfidf` allows users to perform TF-IDF analysis on character-related data, providing insights into the relative importance of terms within a given text compared to the corpus. This is particularly useful for identifying descriptors or actions uniquely associated with specific characters, as well as terms that distinguish a particular text from the broader dataset.

#### Use Case: Character-Level Analysis
Users can apply TF-IDF at the character level to identify terms that uniquely characterize individuals within a text. For example:
- In a historical novel, TF-IDF might highlight words like "chivalrous" or "knight" as distinctive for a particular character compared to others in the same text.
- Conversely, applying TF-IDF across multiple texts could reveal terms commonly associated with a particular character archetype or trope.

#### Use Case: Trends Across Texts
TF-IDF is also valuable for tracking trends in descriptors or actions over time or across genres. For example:
- Researchers might use TF-IDF to identify how descriptors for characters of certain racial or ethnic backgrounds change across decades or literary movements.
- By comparing the TF-IDF scores of racializing terms across texts, users can quantitatively measure shifts in the prominence of certain ethnographic markers or slurs.

To run a TF-IDF analysis, users can use the `run_tfidf` function, specifying the target data (e.g., character actions, possessions, or modifiers) and the desired scope (e.g., per-text or corpus-wide). This analysis provides a structured way to uncover distinctive language patterns, offering valuable context for broader literary trends.


### 5) Collocation Analysis for Contextual Understanding

The function `run_collocation` facilitates collocation analysis, which identifies words that frequently co-occur within a specified context window around a target term. This analysis provides insights into the relationships and associations between words, making it an essential tool for understanding the contextual usage of descriptors or racializing terms in literary texts.

#### Use Case: Racialization Contexts
Collocation analysis can help reveal the nuances of how racializing terms are used in texts. For instance:
- By analyzing collocations of a racializing term, users can uncover patterns of positive, neutral, or negative associations, shedding light on how specific groups are represented.
- For example, examining the collocates of terms like "exotic" or "savage" might reveal whether they are frequently associated with descriptors of beauty, violence, or other themes.

#### Use Case: Character Relationships and Networks
Collocation analysis also provides insights into the relationships between characters, actions, and their surroundings. For instance:
- Users might examine how often a character's name collocates with specific actions or settings, revealing implicit biases in how different characters are portrayed.
- Similarly, researchers can explore how major versus minor characters differ in their networks of collocated terms, uncovering disparities in narrative focus or complexity.

The `run_collocation` function allows users to specify the context window size and the target data for analysis (e.g., modifiers or actions). Results can be visualized using network graphs or heatmaps to illustrate co-occurrence patterns, offering a richer understanding of the contextual dynamics within texts.

# Documentation of any technical decisions or challenges encountered and how they were addressed

Glossary of Racializing Terms: There are not many formally recognized sources of ethnographic terms used to demarcate racial or ethnic identity. While some official sources (say, census designations) exist, these formal sources often do not capture the full breadth of terms used in literary works to indirectly or subtly classify a character's racial background. Similarly, lists of contemporary terms used to refer to one's race may not be sufficient to capture the historical breadth of terms used to racialize characters (either pejoratively or formally). As a makeshift measure, we relied on an informal web source (the "Racial Slurs Database") to see how, and to what extent, characters were ethnically characterized (with either the slurs themselves, or with more neutral identity markers, like "American"). However, we recognize the limitations posed by this approach, and aim to incorporate more robust lists of identity markers in the future.

In developing the TF-IDF and collocation analysis functions, several key technical decisions ensured robust and scalable performance. We integrated metadata with corpus files by matching filenames, using stripped extensions to address inconsistencies, and provided detailed warnings for mismatches. Token preprocessing involved standardizing text by converting to lowercase, removing punctuation, filtering stop words, and excluding numeric tokens. For TF-IDF, we leveraged the `tidytext` package to efficiently calculate scores grouped by metadata, and visualized results using `ggplot2` to highlight term significance across groups. Collocation analysis identified co-occurring terms within a user-defined window, with statistical significance tested using chi-square, ensuring meaningful contextual insights. Challenges included handling missing or noisy data, ensuring metadata alignment, and managing computational overhead for large-scale analysis. These were resolved through robust filtering, informative error messages, and optimized batching for scalability. Collocation results were refined by filtering for statistical significance and excluding short or irrelevant tokens. Future enhancements could include parallel processing for speed, dynamic grouping for flexibility, and advanced collocation metrics like mutual information to provide deeper insights into term relationships. These improvements would expand the utility of the package for larger and more diverse datasets.

# Final Implementation v.s Initial Proposal 

Our original proposal envisioned this package as one that enabled researchers in the computational humanities to consolidate and summarize outputs from BookNLP's parse on a corpus, and conduct keyword analysis and frequency mapping on the results of this wrangled output. We believe that our primary reading functions perfom the former, whereas the key analytical functions (foudn in analysis_funcs, such as TF-IDF analysis on corpus-subsets and collocate analysis) allow users to perform the latter in ways that are customizable, depending on the researcher's background interest (e.g., if they are interested in the relation between literary trends and author's temporal/racial background). Our initial proposal was also interested in seeing how this package might enable users to pick up on the usage of cultural and ethnographic terms in literary texts. While this initiative was a bit impeded by the lack of a formal database on such terms, our incorporation of cross-references to informal, webscraped sources allows for an ad-hoc pipeline of such analysis. 

# Clear demonstration of each team memberâ€™s contributions
Kate and Sang contributed equally to the development of the package, each bringing complementary expertise to its design and functionality. Kate focused on processing and wrangling BookNLP files, creating the foundational functions that generate character-level dataframes essential for downstream analysis. She also developed functions for extracting sentiment from character text and cross-referencing this data with a database of racializing terms, ensuring a thorough and nuanced exploration of representation. Sang, in turn, wrote the primary analytical functions, which calculate time-specific TF-IDF scores and perform collocation analysis, enabling the detection of key trends and contextual relationships in the texts. Both Sang and Kate collaborated closely on writing the documentation and tests for their respective functions, ensuring a cohesive and well-documented package. While Kate initiated the project, drawing on her research interests and gathering the dataset used for testing, Sang's analytical contributions were instrumental in enabling the advanced insights the package offers. Together, their balanced collaboration resulted in a powerful tool for literary analysis, reflecting their shared commitment to rigorous and impactful scholarship.

# Package Solving Use Cases 

For researchers in the computational humanities, common challenges include the following: 

#### 1) It's difficult to generate insights from multiple texts, while retaining detail at the sub-text level.
This package addresses this issue by processing BookNLP parses of a corpus, while also building in corpus-level analytical functions. As a result, users can look at the character-level data within a text, if it poses avenues of additional interest. They can also scale-up this analysis, to generate insights at the corpus-level.

#### 2) Identifying Distinctive Terms and Contextual Relationships Across Texts

Example 1: TF-IDF Analysis. One challenge researchers face is identifying terms that are significant within a text or character's narrative relative to the broader corpus. Using the package's TF-IDF function, researchers can determine how often specific terms, such as "heroic" or "savage," are uniquely associated with characters or themes in a given text. For instance, by analyzing a 19th-century corpus, TF-IDF might reveal that "exotic" frequently describes characters of certain racial or ethnic backgrounds in colonial literature, providing insights into historical patterns of representation. The results can be visualized to compare term significance across time or authorship, offering a clearer understanding of how descriptive language evolves.

Example 2: Collocation Analysis. Collocation analysis helps uncover the contextual relationships of terms by identifying words that frequently co-occur within a specified window. For example, a researcher examining racialized terms might find that "savage" often appears with "uncivilized" or "tribe" in adventure novels, revealing implicit biases and thematic associations. By comparing collocates of these terms across texts or time periods, the package can highlight shifts in narrative framing, such as the move from overtly racialized language to subtler forms of stereotyping. This contextual insight allows researchers to explore how word associations contribute to broader narrative dynamics and representation. 

Both TF-IDF and collocation analysis demonstrate the package's ability to generate scalable, detailed insights that bridge text-level and corpus-level analyses, addressing key challenges in computational literary research.

#### 4) It's difficult to manage character-level data in computational approaches. 
As a result of the granularity-breadth tradeoff, many researchers in computational literature are not able to keep track of changes at the character level, or monitor how individual characters are characterized. This package documents both minor and major characters within a given text, and allows researchers to plot how character majorness relates to the sentiment of their descriptions. Consequently, researchers will have access to not only the formal elements of a text (e.g., language/syntactic relations), but also more subtle methods of characterization. 

The primary extension to this project would be the ability to process larger-scale corpus. The example corpus listed here is a very small subset (~6 texts) from the larger Chicago corpus (~100 texts). The current analytical functions here can process small-scale texts easily, but as the number of texts and size of constituent texts increases, standard tidyverse functions will not be capable of efficiently processing and storing this data. By taking advantage of R's existing interfaces with more efficient, lower-level languages (e.g., C++ with Rcpp) might be able to perform some of the computational legwork associated with processing a larger corpus. (Note that this package's focus on producing legible results and integration into a larger humanistic workflow would dissuade us from authoring the package primarily with Rcpp. As a result, the current iteration of this project focuses on legibility and quality of output, not necessarily the immediate ability to increase analytical scale.)
