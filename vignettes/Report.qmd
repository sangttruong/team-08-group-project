---
title: "Report"
author: "Kate Li"
date: "2024-12-8"
format: html
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{quarto::html}
  %\VignetteEncoding{UTF-8}
---

# Section 1: Introduction
## Package Purpose & Problem Solving

The computational humanities is interested in empirically assessing trends and methods of representation in literature, in ways that traditional close-reading methods cannot. However, as the number of texts subject to purview (that is, as the size of one's "corpus") grows, research methods abstract enough to assess trends across a corpus in its entirety tend to lose the precision required for divulging the granularity of individual texts. Our package aims to address this issue by providing functions that allow users to 1) analyze character development at the textual level, as well as 2) analyzing *trends* in character development at the corpus level. This package does so through the presumption that users already have access to a corpus and have run NLP pipelines (in particular, BookNLP) on the corpus; this package takes in, wrangles, and visualizes data associated with BookNLP parses on a given corpus, allowing users to more directly visualize how individual characters are characterized, and what that might say about thematic trends within their selected corpus. 

# Description of the main functionality and features implemented

This package's main functions include reading in BookNLP-parses of a corpus (stored in the file *read_corpus*), wrangling and processing this data to generate key attributes for every character in every document (stored in the file *rsdb*), conducting a range of textual analyses on these attributes such as sentiment analysis, co-reference against a webscraped database on racializing terms, collocate analysis (as in sentiment.R and analysis_funcs.R), and visualization of the resulting output. This host of functions aims to enable users to generate key findings on the nature of characterization in their corpus, beyond the higher-level linguistic patterns that standard text mining methods could produce. 

# Detailed examples showing how to use the package’s key functions

The recommended pipeline for analysis is as follows:

### 1) Read in BookNLP Parses & Generate Character Data 
Given a modest corpus (e.g., like one stored under inst/extdata/examplecorpus) containing documents parsed by BookNLP (e.g., JSON files ending with the file extension ".book"), we recommend first running the function "getbook" on the corpus. 

This function will return a list of dataframes containing data on a text's characters (where each dataframe corresponds to one text). Data includes a list of actions performed by the character, done by the character, a list of their possessions, as well as modifiers used to describe them. These dataframes can be either saved to the user's local desktop, or used for downstream analysis (see following steps).

The following functions are not sequential: users can choose whichever function to run in whichever order, depending on their research interests.

### 2) Analyze Trends in Racialization
The function "grab_racialmods" takes in character-level data at the text-level: it accepts one dataframe returned by "getbook" (e.g., one entry in the list output). Users can choose to loop grab_racialmods to apply the function to all texts within their corpus, but the function is currently limited to accepting only one document at a time as an input parameter. 

This function cross-references the modifiers used to describe characters against an existing database containing a list of racializing terms (e.g., ethnographic markers, racial slurs, etc). Users can obtain a list of racializing terms that are used within a given document, and the characters that these terms were used to describe. 

### 3) Sentiment Analysis
The function "grab_sentiment" takes in a list of dataframes containing character data (e.g., the output of "getbooks") and returns a list of modified dataframes, where added variables describe the sentiment of each character's descriptions/actions. Users can choose to perform sentiment analysis on either characters' actions, modifiers, possessions, etc., by specifying one of four parameters in the function (see /man for function documentation). 

Users can run "plot_sentiment" on the output generated by grab_sentiment to visualize the results of the former (e.g., plot_sentiment(grab_sentiment(), measure = mean, charalevel = 5). It generates simple scatter plots relating the majorness of characters in the corpus, to the manners in which they are sentimentally described. Since plot_sentiment takes the output of grab_sentiment() as an input, we recommend running grab_sentiment() prior to running plot_sentiment. 

# Documentation of any technical decisions or challenges encountered and how they were addressed

Glossary of Racializing Terms: There are not many formally recognized sources of ethnographic terms used to demarcate racial or ethnic identity. While some official sources (say, census designations) exist, these formal sources often do not capture the full breadth of terms used in literary works to indirectly or subtly classify a character's racial background. Similarly, lists of contemporary terms used to refer to one's race may not be sufficient to capture the historical breadth of terms used to racialize characters (either pejoratively or formally). As a makeshift measure, we relied on an informal web source (the "Racial Slurs Database") to see how, and to what extent, characters were ethnically characterized (with either the slurs themselves, or with more neutral identity markers, like "American"). However, we recognize the limitations posed by this approach, and aim to incorporate more robust lists of identity markers in the future. 

# Discussion of how the final implementation relates to the original proposal

Our original proposal envisioned this package as one that enabled researchers in the computational humanities to consolidate and summarize outputs from BookNLP's parse on a corpus, and conduct keyword analysis and frequency mapping on the results of this wrangled output. We believe that our primary reading functions perfom the former, whereas the key analytical functions (foudn in analysis_funcs, such as TF-IDF analysis on corpus-subsets and collocate analysis) allow users to perform the latter in ways that are customizable, depending on the researcher's background interest (e.g., if they are interested in the relation between literary trends and author's temporal/racial background). Our initial proposal was also interested in seeing how this package might enable users to pick up on the usage of cultural and ethnographic terms in literary texts. While this initiative was a bit impeded by the lack of a formal database on such terms, our incorporation of cross-references to informal, webscraped sources allows for an ad-hoc pipeline of such analysis. 

# Clear demonstration of each team member’s contributions

Kate wrote functions to process and wrangle BookNLP files to generate character-level dataframes, which are used for the downstream analysis. Sang wrote the primary analytical functions (which generated time-specific TF-IDFs and performed collocate analysis). Kate also wrote functions to extract sentiment from character text data, and cross-reference this character-level data with a database on racializing terms. Both Sang and Kate wrote the documentation and tests associated with their respective functions. Because this project theme is more within Kate's research interests and background, Kate initiated the construction of these functions and gathered the data used to test this package. 

# Examples of the package solving the intended use case

For researchers in the computational humanities, common challenges include the following: 

#### 1) It's difficult to generate insights from multiple texts, while retaining detail at the sub-text level.
This package addresses this issue by processing BookNLP parses of a corpus, while also building in corpus-level analytical functions. As a result, users can look at the character-level data within a text, if it poses avenues of additional interest. They can also scale-up this analysis, to generate insights at the corpus-level.

#### 2) 


# Discussion of potential future enhancements or extensions

The primary extension to this project would be the ability to process larger-scale corpus. The example corpus listed here is a very small subset (~6 texts) from the larger Chicago corpus (~100 texts). The current analytical functions here can process small-scale texts easily, but as the number of texts and size of constituent texts increases, standard tidyverse functions will not be capable of efficiently processing and storing this data. By taking advantage of R's existing interfaces with more efficient, lower-level languages (e.g., C++ with Rcpp) might be able to perform some of the computational legwork associated with processing a larger corpus. (Note that this package's focus on producing legible results and integration into a larger humanistic workflow would dissuade us from authoring the package primarily with Rcpp. As a result, the current iteration of this project focuses on legibility and quality of output, not neccesarily the immediate ability to increase analyatical scale.)
